{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "nuahiNV7oXfb"
      },
      "source": [
        "# Experiment 2 & 4 - Policy Iteration and Dynamic Programming"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "1g6ecaX-n5wN"
      },
      "source": [
        "Note : If we only print the `optimal policy` then this answers the `Policy Iteration` experiment . \n",
        "\n",
        "And if we print both the `optimal policy` and `value function`, this answers the `Dynamic Programming` experiment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jiiX0Z9IGzhZ",
        "outputId": "99ea4845-ceb0-4655-8437-7cfe24f0e528"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(16,)\n",
            "(16,)\n",
            "[[1. 0. 0. 0.]\n",
            " [0. 0. 0. 1.]\n",
            " [1. 0. 0. 0.]\n",
            " [0. 0. 0. 1.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [0. 0. 0. 1.]\n",
            " [0. 1. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [0. 0. 1. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [1. 0. 0. 0.]]\n",
            "[0.06889086 0.06141454 0.07440974 0.0558073  0.0918545  0.\n",
            " 0.1122082  0.         0.14543633 0.24749694 0.29961758 0.\n",
            " 0.         0.37993589 0.63902014 0.        ]\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
            "  and should_run_async(code)\n"
          ]
        }
      ],
      "source": [
        "import gym\n",
        "import numpy as np\n",
        "\n",
        "# create the environment\n",
        "env = gym.make('FrozenLake-v1',new_step_api=True)\n",
        "\n",
        "# set the discount factor\n",
        "gamma = 0.9\n",
        "\n",
        "# initialize the policy to a uniform distribution over actions for each state\n",
        "num_states = env.observation_space.n\n",
        "num_actions = env.action_space.n\n",
        "policy = np.ones((num_states, num_actions)) / num_actions\n",
        "\n",
        "# define the policy evaluation function\n",
        "def policy_evaluation(policy, env, gamma, theta):\n",
        "    # initialize the value function to zero for all states\n",
        "    V = np.zeros(num_states)\n",
        "  \n",
        "    while True:\n",
        "        delta = 0\n",
        "        # for each state, update the value function\n",
        "        for s in range(num_states):\n",
        "            v = V[s]\n",
        "            # calculate the value of each action in the current state\n",
        "            q = np.zeros(num_actions)  \n",
        "            for a in range(num_actions):\n",
        "                for prob, next_state, reward, done in env.P[s][a]:\n",
        "                    q[a] += prob * (reward + gamma * V[next_state])\n",
        "            # update the value of the current state\n",
        "            V[s] = np.dot(policy[s], q)\n",
        "            # calculate the maximum change in the value function\n",
        "            delta = max(delta, abs(v - V[s]))\n",
        "        # check if the maximum change is below the threshold\n",
        "        if delta < theta:\n",
        "            break\n",
        "    return V\n",
        "\n",
        "# define the policy improvement function\n",
        "def policy_improvement(policy, env, gamma, V):\n",
        "    # initialize the policy to be greedy with respect to the current value function\n",
        "    new_policy = np.zeros((num_states, num_actions))\n",
        "    for s in range(num_states):\n",
        "        # calculate the value of each action in the current state\n",
        "        q = np.zeros(num_actions)\n",
        "        for a in range(num_actions):\n",
        "            for prob, next_state, reward, done in env.P[s][a]:\n",
        "                q[a] += prob * (reward + gamma * V[next_state])\n",
        "        # update the policy to choose the action with the highest value\n",
        "        best_action = np.argmax(q)\n",
        "        new_policy[s][best_action] = 1\n",
        "    return new_policy\n",
        "\n",
        "# perform policy iteration\n",
        "theta = 1e-8  # threshold for convergence\n",
        "while True:\n",
        "    # evaluate the current policy\n",
        "    V = policy_evaluation(policy, env, gamma, theta)\n",
        "    # improve the policy\n",
        "    new_policy = policy_improvement(policy, env, gamma, V)\n",
        "    # check if the policy has converged\n",
        "    if np.array_equal(policy, new_policy):\n",
        "        break\n",
        "    policy = new_policy\n",
        "\n",
        "# print the final policy\n",
        "print(policy)\n",
        "# print the value function\n",
        "print(V)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "EE84Qzz9u26E"
      },
      "source": [
        "# Exp 3 - Implement Markov Decision Process\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jyUk_5BxyhFN",
        "outputId": "a760bcff-ec61-4df8-8921-b94e2affaae9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[0.06848032 0.06111567 0.07422254 0.05560469 0.09153995 0.\n",
            " 0.11212558 0.         0.14522151 0.24737863 0.29954442 0.\n",
            " 0.         0.37986011 0.63898452 0.        ]\n",
            "[[1. 0. 0. 0.]\n",
            " [0. 0. 0. 1.]\n",
            " [1. 0. 0. 0.]\n",
            " [0. 0. 0. 1.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [0. 0. 0. 1.]\n",
            " [0. 1. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [0. 0. 1. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [1. 0. 0. 0.]]\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
            "  and should_run_async(code)\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import gym\n",
        "\n",
        "num_states = env.observation_space.n\n",
        "num_actions = env.action_space.n\n",
        "# policy = np.ones((num_states, num_actions)) / num_actions\n",
        "def value_iteration(env, theta=0.0001, gamma=0.9):\n",
        "    # Initialize the value function to zero\n",
        "    V = np.zeros(num_states)\n",
        "    \n",
        "    # Loop until convergence\n",
        "    while True:\n",
        "        delta = 0\n",
        "        \n",
        "        # For each state s\n",
        "        for s in range(num_states):\n",
        "            v = V[s]\n",
        "            \n",
        "            # Compute the new value for the state s\n",
        "            q = np.zeros(num_actions)\n",
        "            for a in range(num_actions):\n",
        "                for prob, next_state, reward, done in env.P[s][a]:\n",
        "                    q[a] += prob * (reward + gamma * V[next_state])\n",
        "            \n",
        "            V[s] = np.max(q)\n",
        "            delta = max(delta, np.abs(v - V[s]))\n",
        "        \n",
        "        # Check for convergence\n",
        "        if delta < theta:\n",
        "            break\n",
        "    \n",
        "    # Compute the optimal policy\n",
        "    policy = np.zeros((num_states, num_actions))\n",
        "    for s in range(num_states):\n",
        "        q = np.zeros(num_actions)\n",
        "        for a in range(num_actions):\n",
        "            for prob, next_state, reward, done in env.P[s][a]:\n",
        "                q[a] += prob * (reward + gamma * V[next_state])\n",
        "        best_action = np.argmax(q)\n",
        "        policy[s, best_action] = 1\n",
        "    \n",
        "    return V, policy\n",
        "\n",
        "env = gym.make('FrozenLake-v1', new_step_api = True)\n",
        "V, policy = value_iteration(env)\n",
        "print(V)\n",
        "print(policy)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DhJxnBpgzGYl",
        "outputId": "17a41675-6003-414b-aaa0-7bfc567d4c74"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
            "  and should_run_async(code)\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "(16,)"
            ]
          },
          "execution_count": 18,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "V.shape"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "pZP2zpGsemNU"
      },
      "source": [
        "# Exp 5 - Q - Learning Algorithm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2fcvBvOgexUE",
        "outputId": "174e33db-cd8c-43fb-d9e7-6ba2133779d0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[[0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]]\n",
            "Exploration: 23635\n",
            "Exploitation: 94305\n",
            "Q-value function:\n",
            "[[1.88624562e-04 8.05946586e-05 9.15627970e-05 2.00052179e-04]\n",
            " [5.99210641e-05 2.69268163e-04 3.29664504e-04 1.54942093e-04]\n",
            " [2.73714751e-04 3.00336048e-04 8.77229575e-03 2.94591088e-04]\n",
            " [6.07419982e-06 3.80499444e-04 9.42439263e-05 8.21875857e-04]\n",
            " [1.61126843e-04 2.59795286e-04 4.44321761e-04 1.95897742e-04]\n",
            " [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]\n",
            " [3.10004235e-03 1.65295222e-04 7.97674621e-05 3.21918023e-05]\n",
            " [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]\n",
            " [4.88305201e-04 8.21775624e-06 2.31510271e-05 4.38130409e-04]\n",
            " [1.70749419e-02 9.70738590e-02 1.03774512e-02 1.39686736e-02]\n",
            " [3.09662458e-03 4.91901259e-02 6.97762023e-02 2.50989390e-05]\n",
            " [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]\n",
            " [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]\n",
            " [8.47805122e-03 3.28061304e-03 3.50264994e-02 2.71622821e-04]\n",
            " [1.86987155e-01 7.15121670e-02 8.26328606e-01 1.31139541e-01]\n",
            " [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]]\n",
            "\n",
            "Optimal policy:\n",
            "[[3 2 2 3]\n",
            " [2 0 0 0]\n",
            " [0 1 2 0]\n",
            " [0 2 2 0]]\n"
          ]
        }
      ],
      "source": [
        "import gym\n",
        "import numpy as np\n",
        "\n",
        "# Create the FrozenLake environment\n",
        "env = gym.make('FrozenLake-v1')\n",
        "\n",
        "# Define the Q-value function as a matrix with size (num_states, num_actions)\n",
        "num_states = env.observation_space.n\n",
        "num_actions = env.action_space.n\n",
        "Q = np.zeros((num_states, num_actions))\n",
        "\n",
        "\n",
        "# Define the parameters of the Q-learning algorithm\n",
        "alpha = 0.8  # learning rate\n",
        "gamma = 0.7  # discount factor\n",
        "epsilon = 0.2  # exploration probability\n",
        "\n",
        "explo = 0\n",
        "explt = 0\n",
        "\n",
        "# Define a function to select an action based on the Q-value function and exploration-exploitation strategy\n",
        "def select_action(state):\n",
        "    if np.random.rand() < epsilon:\n",
        "        global explo\n",
        "        explo += 1\n",
        "        return env.action_space.sample()  # explore - a random action is chosen\n",
        "    else:\n",
        "        global explt\n",
        "        explt += 1\n",
        "        return np.argmax(Q[state])  # exploit - action with the highest Q-value in the current state is chosen\n",
        "\n",
        "# Define the main loop of the Q-learning algorithm\n",
        "num_episodes = 10000\n",
        "for episode in range(num_episodes):\n",
        "    state = env.reset()\n",
        "    done = False\n",
        "    while not done:\n",
        "        action = select_action(state)\n",
        "        next_state, reward, done, info = env.step(action)\n",
        "        Q[state, action] += alpha * (reward + gamma * np.max(Q[next_state]) - Q[state, action])  # update the Q-value function\n",
        "        state = next_state\n",
        "\n",
        "# Print the learned Q-value function and optimal policy\n",
        "print(f\"Exploration: {explo}\")\n",
        "print(f\"Exploitation: {explt}\")\n",
        "print(\"Q-value function:\")\n",
        "print(Q)\n",
        "optimal_policy = np.argmax(Q, axis=1)\n",
        "print(\"\\nOptimal policy:\")\n",
        "print(optimal_policy.reshape((4,4)))\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "k4azpbXlsJmj"
      },
      "source": [
        "# 6 - Bellman Equation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "at-s1H5t5exo"
      },
      "outputs": [],
      "source": [
        "import gym\n",
        "import numpy as np\n",
        "\n",
        "# Create the FrozenLake environment\n",
        "env = gym.make('FrozenLake-v1')\n",
        "\n",
        "# Define the value function as a vector with size (num_states)\n",
        "num_states = env.observation_space.n\n",
        "V = np.zeros(num_states)\n",
        "print(\"Initial Value Function\")\n",
        "print(V.reshape(4, 4))\n",
        "print()\n",
        "\n",
        "# Define the parameters of the value iteration algorithm\n",
        "gamma = 0.99  # discount factor\n",
        "epsilon = 1e-8  # convergence threshold\n",
        "num_iterations = 100000  # maximum number of iterations\n",
        "\n",
        "# Define a function to update the value of a state based on the Bellman equation\n",
        "def bellman_update(V, state, gamma):\n",
        "    action_values = np.zeros(env.action_space.n)\n",
        "    for action in range(env.action_space.n):\n",
        "        for prob, next_state, reward, done in env.P[state][action]:\n",
        "            action_values[action] += prob * (reward + gamma * V[next_state])\n",
        "    return np.max(action_values)\n",
        "\n",
        "# Run the value iteration algorithm\n",
        "for i in range(num_iterations):\n",
        "    delta = 0\n",
        "    for state in range(num_states):\n",
        "        v = V[state]\n",
        "        V[state] = bellman_update(V, state, gamma)\n",
        "        delta = max(delta, abs(v - V[state]))\n",
        "    if delta < epsilon:\n",
        "        break\n",
        "\n",
        "# Print the learned value function and optimal policy\n",
        "print(\"Value function:\")\n",
        "print(V.reshape(4, 4))\n",
        "optimal_policy = np.zeros(num_states, dtype=np.int)\n",
        "for state in range(num_states):\n",
        "    action_values = np.zeros(env.action_space.n)\n",
        "    for action in range(env.action_space.n):\n",
        "        for prob, next_state, reward, done in env.P[state][action]:\n",
        "            action_values[action] += prob * (reward + gamma * V[next_state])\n",
        "    optimal_policy[state] = np.argmax(action_values)\n",
        "print(\"\\nOptimal policy:\")\n",
        "print(optimal_policy.reshape((4, 4)))"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "9j24wFIE3Nl0"
      },
      "source": [
        "# Monte Carlo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lfvsHEEz3PTb",
        "outputId": "6897c0aa-a5e9-4b5d-f61a-72ba05414c42"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Q-value function:\n",
            "[[0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]]\n",
            "\n",
            "Optimal policy:\n",
            "[[0 0 0 0]\n",
            " [0 0 0 0]\n",
            " [0 0 0 0]\n",
            " [0 0 0 0]]\n"
          ]
        }
      ],
      "source": [
        "import gym\n",
        "import numpy as np\n",
        "\n",
        "# Create the FrozenLake environment\n",
        "env = gym.make('FrozenLake-v1')\n",
        "\n",
        "# Define the parameters of the Monte Carlo Control algorithm\n",
        "num_episodes = 10000\n",
        "gamma = 0.9  # discount factor\n",
        "\n",
        "# Define the Q-value function as a matrix with size (num_states, num_actions)\n",
        "num_states = env.observation_space.n\n",
        "num_actions = env.action_space.n\n",
        "Q = np.zeros((num_states, num_actions))\n",
        "\n",
        "# Define a dictionary to store the returns for each state-action pair\n",
        "returns = {}\n",
        "\n",
        "# Define a function to select an action based on the Q-value function and exploration-exploitation strategy\n",
        "def select_action(state, epsilon):\n",
        "    if np.random.rand() < epsilon:\n",
        "        return env.action_space.sample()  # explore\n",
        "    else:\n",
        "        return np.argmax(Q[state])  # exploit\n",
        "\n",
        "# Define the main loop of the Monte Carlo Control algorithm\n",
        "for episode in range(num_episodes):\n",
        "    # Initialize episode variables\n",
        "    state = env.reset()\n",
        "    done = False\n",
        "    episode_history = []\n",
        "\n",
        "    # Generate an episode by following the current policy\n",
        "    while not done:\n",
        "        action = select_action(state, epsilon=1/(episode+1))  # decaying epsilon-greedy exploration\n",
        "        next_state, reward, done, _ = env.step(action)\n",
        "        episode_history.append((state, action, reward))\n",
        "        state = next_state\n",
        "\n",
        "    # Update the Q-value function using the episode history\n",
        "    G = 0  # total discounted reward\n",
        "    for t in reversed(range(len(episode_history))):\n",
        "        state, action, reward = episode_history[t]\n",
        "        G = gamma * G + reward\n",
        "        state_action = (state, action)\n",
        "        if state_action not in [(x[0], x[1]) for x in episode_history[0:t]]:\n",
        "            if state_action not in returns:\n",
        "                returns[state_action] = [G]\n",
        "            else:\n",
        "                returns[state_action].append(G)\n",
        "            Q[state][action] = np.mean(returns[state_action])\n",
        "\n",
        "# Print the learned Q-value function and optimal policy\n",
        "print(\"Q-value function:\")\n",
        "print(Q)\n",
        "optimal_policy = np.argmax(Q, axis=1)\n",
        "print(\"\\nOptimal policy:\")\n",
        "print(optimal_policy.reshape((4,4)))\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "ey9TlI-CrCWa"
      ],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
